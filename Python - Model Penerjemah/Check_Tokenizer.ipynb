{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(file_path):\n",
    "    return pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Load the tokenizer for a specific language\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    return Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# Function to view original and tokenized text\n",
    "def view_text_and_tokenized_text(dataset, tokenizer_src, tokenizer_tgt, index):\n",
    "    src_text = dataset.iloc[index]['Maanyan']\n",
    "    tgt_text = dataset.iloc[index]['Indonesia']\n",
    "\n",
    "    src_tokens = tokenizer_src.encode(src_text)\n",
    "    tgt_tokens = tokenizer_tgt.encode(tgt_text)\n",
    "\n",
    "    print(\"Original Maanyan Text:\")\n",
    "    print(src_text)\n",
    "    print(\"\\nTokenized Maanyan Text:\")\n",
    "    print(src_tokens.ids)\n",
    "    \n",
    "    print(\"\\nOriginal Indonesia Text:\")\n",
    "    print(tgt_text)\n",
    "    print(\"\\nTokenized Indonesia Text:\")\n",
    "    print(tgt_tokens.ids)\n",
    "\n",
    "# Function to tokenize input text and view token IDs\n",
    "def tokenize_and_view(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    print(\"Input Text:\")\n",
    "    print(text)\n",
    "    print(\"\\nToken IDs:\")\n",
    "    print(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Maanyan Text:\n",
      "amun naun kaiuh iwara upi yiru anri aratini\n",
      "\n",
      "Tokenized Maanyan Text:\n",
      "[17, 75, 122, 42, 55, 6, 19, 66]\n",
      "\n",
      "Original Indonesia Text:\n",
      "jika kamu dapat memberitahukan mimpi itu dan maknanya\n",
      "\n",
      "Tokenized Indonesia Text:\n",
      "[57, 10, 39, 45, 46, 6, 4, 77]\n",
      "=========================================================================\n",
      "Input Text:\n",
      "tolong\n",
      "\n",
      "Token IDs:\n",
      "[231]\n",
      "=========================================================================\n",
      "Input Text:\n",
      "mulek\n",
      "\n",
      "Token IDs:\n",
      "[0]\n",
      "=========================================================================\n",
      "Input Text:\n",
      "aku terkejut\n",
      "\n",
      "Token IDs:\n",
      "[7, 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV dataset\n",
    "csv_file_path = 'D:\\Oscar Main Base\\File Online\\Aplikasi_Penerjemah\\Develop\\pytorch-transformer-main\\datasets\\datasetma4.csv'\n",
    "dataset = load_csv_dataset(csv_file_path)\n",
    "\n",
    "# Load the tokenizers\n",
    "tokenizer_src_path = 'tokenizer_Maanyan.json'\n",
    "tokenizer_tgt_path = 'tokenizer_Indonesia.json'\n",
    "tokenizer_src = load_tokenizer(tokenizer_src_path)\n",
    "tokenizer_tgt = load_tokenizer(tokenizer_tgt_path)\n",
    "\n",
    "# View original and tokenized text for a specific index\n",
    "index_to_view = 20  # Change this to the index you want to view\n",
    "view_text_and_tokenized_text(dataset, tokenizer_src, tokenizer_tgt, index_to_view)\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"tolong\"\n",
    "# Tokenize and view token IDs\n",
    "# tokenize_and_view(input_text, tokenizer_src)\n",
    "tokenize_and_view(input_text, tokenizer_tgt)\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"mulek\"\n",
    "# Tokenize and view token IDs\n",
    "# tokenize_and_view(input_text, tokenizer_src)\n",
    "tokenize_and_view(input_text, tokenizer_tgt)\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"aku terkejut\"\n",
    "# Tokenize and view token IDs\n",
    "# tokenize_and_view(input_text, tokenizer_src)\n",
    "tokenize_and_view(input_text, tokenizer_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from train import translate_input_string_bleu, get_model, get_ds_csv, translate_input_string\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, get_weights_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 22\n",
      "Max length of target sentence: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "# config['model_basename'] = \"tmodel_eight_four_id_\"\n",
    "config['model_basename'] = \"tmodel_eight_four_\"\n",
    "# Training bahasa target menjadi bahasa Maanyan\n",
    "# config['lang_src'] = \"Indonesia\"\n",
    "# config['lang_tgt'] = \"Maanyan\"\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds_csv(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"60\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: hanyu\n",
      "Translation: kamu kamu kamu dan kamu kamu kamu kamu kamu kamu\n"
     ]
    }
   ],
   "source": [
    "input_string = \"hanyu\"\n",
    "translation = translate_input_string(model, input_string, tokenizer_src, tokenizer_tgt, config['seq_len'], device)\n",
    "print(f\"Input String: {input_string}\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persentase token cocok: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_translation(reference, translation):\n",
    "\n",
    "    reference_tokens = reference.lower().split()  # Ubah ke huruf kecil dan pecah menjadi token\n",
    "    translation_tokens = translation.lower().split()\n",
    "\n",
    "    # Hitung jumlah token yang cocok antara referensi dan terjemahan\n",
    "    matching_tokens = set(reference_tokens) & set(translation_tokens)\n",
    "\n",
    "    # Hitung persentase token yang cocok\n",
    "    matching_percentage = (len(matching_tokens) / len(reference_tokens)) * 100.0\n",
    "\n",
    "    return matching_percentage\n",
    "\n",
    "# Contoh penggunaan\n",
    "reference = \"kamu\"\n",
    "percentage = evaluate_translation(reference, translation)\n",
    "print(f\"Persentase token cocok: {percentage}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
